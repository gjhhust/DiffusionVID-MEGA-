MODEL:
  META_ARCHITECTURE: "DiffusionDet"
  WEIGHT: "models/diffdet_coco_swinbase.pth" # "models/swin_base_patch4_window7_224_22k.pkl"
  BACKBONE:
    NAME: "build_swintransformer_fpn_backbone"
  SWIN:
    SIZE: B-22k
    OUT_FEATURES: (1,2,3)
  FPN:
    IN_FEATURES: ["swin1", "swin2", "swin3" ]
    OUT_CHANNELS: 256
  ROI_HEADS:
    IN_FEATURES: ["p3", "p4", "p5"]  # ["p2", "p3", "p4", "p5"]
  ROI_BOX_HEAD:
    POOLER_TYPE: "ROIAlignV2"
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
  DiffusionDet:
    NUM_PROPOSALS: 300  # 500
    NUM_CLASSES: 30
    HIDDEN_DIM: 256
    NUM_HEADS: 3
    NUM_HEADS_LOCAL: 1
    SAMPLE_STEP: 1
  VID:
    METHOD: "diffusion"
    ROI_BOX_HEAD:
      ATTENTION:
        ENABLE: False  # local attention
        STAGE: 0  # local attention stages
    MEGA:
      MIN_OFFSET: -0
      MAX_OFFSET: 3
      ALL_FRAME_INTERVAL: 4
      KEY_FRAME_LOCATION: 0
      SHUFFLED_CUR_TEST : False
      LOCAL:
        ENABLE: False  # local frame access on training phase
      GLOBAL:
        ENABLE: True  # global attention & memory
        RES_STAGE: 1
        SIZE: 24  # global ref frames in initialization
        STOP_UPDATE_AFTER_INIT_TEST: True
      REF_NUM_GLOBAL: 4  # global ref frames in training
      MEMORY_MANAGEMENT_SIZE_TEST: 900
      MEMORY_MANAGEMENT_SIZE_TRAIN: 300
      MEMORY_MANAGEMENT_METRIC: "distance"  # mamba, queue, distance
      MEMORY_MANAGEMENT_TYPE: "greedy"  # once, t
SOLVER:
  OPTIMIZER_TYPE: "adamw" #"adamw" "sgd"
  LR_SCHEDULER_TYPE: "step" # step or cosine
  BASE_LR: 0.0001
  BACKBONE_MULTIPLIER: 0.1
  BIAS_LR_FACTOR: 1.0
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  STEPS: (40000, 60000)
  MAX_ITER: 70000
  WARMUP_ITERS: 20000
  TEST_PERIOD: 10000
  CHECKPOINT_PERIOD: 10000
  ACCUMULATION_STEPS: 2  # batch size multiplier
  BATCH_REUSE_STEPS: 1
DATALOADER:
  SIZE_DIVISIBILITY: 32  # essential because of FPN
  NUM_WORKERS: 8  # 1 for debug
DATASETS:
  TRAIN: ("DET_train_30classes", "VID_train_15frames")
  TEST: ("VID_val_videos",) # ("VID_val_videos",) # ("YouTube_Objects",)
INPUT:
  PIXEL_MEAN: [ 123.675, 116.280, 103.530 ] # RGB
  PIXEL_STD: [ 58.395, 57.120, 57.375 ]
  TO_BGR255: False # pretrained torchvision weight use RGB format.
  TRANSFORM: True
  INFER_BATCH: 4 # must be same with MAX_OFFSET + 1